<!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC"
          crossorigin="anonymous">
    
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet">
  

    <link rel="stylesheet" href="style/style.css">

    <title>Thesis Website</title>
</head>
<body>

    <nav class="navbar navbar-light bg-light-nav-custom">
        <div class="container-fluid">
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarText" aria-controls="navbarText" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarText">
            <ul class="navbar-nav me-auto mb-2 mb-lg-0">
            <li class="nav-item">
                <a class="nav-link active" href="index.html">Main</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="safety_evaluation.html">Safety</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="decision_making.html">Decision Making</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="functions.html">Functions</a>
            </li>
            </ul>
        </div>
        </div>
    </nav>

      <div class="alert alert-danger alert-dismissible fade show alert-sticky" role="alert">
        <strong>Website still in progress, </strong> it will be final on the 21st of June !
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
      </div>

    <div class="container container-box-main border rounded p-4 mt-4 ">
        <div class="row justify-content-center">
            <div class="col-auto">
                <h1>GPTAlly</h1>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-auto">
                <h4> A Safety-Oriented System
                    for Human-Robot Collaboration
                    based on Foundation Models</h4>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-auto">
                <h5><span style="color: rgb(79, 79, 239);"> Brieuc Bastin<sup>1</sup>, Gustavo Alfonso Garcia Ricardez<sup>2</sup>, Lotfi El Hafi<sup>2</sup>, Renaud Ronsse<sup>1</sup>, Benoit Macq<sup>1</sup></span></h5>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-auto">
                <h6><sup>1</sup>UCLouvain, <sup>2</sup>Ritsumeikan</h6>
            </div>
        </div>
        <div class="row justify-content-center mt-3">
            <div class="col-auto">
                <a href="https://github.com/Axtiop/GPTAlly_codebase.git" class="btn btn-light btn-with-logo border">
                        <span>GitHub</span>
                        <i class="fab fa-github"></i> <!-- GitHub icon -->
                </a>
            </div>
            <div class="col-auto">
                <button class="btn btn-light btn-with-logo border">
                    <span>Paper</span>
                    <i class="fas fa-file-pdf"></i> <!-- GitHub icon -->
                </button>
            </div>
            <div class="col-auto">
                <button class="btn btn-light btn-with-logo border">
                    <span>Video</span>
                    <i class="fas fa-video"></i> <!-- GitHub icon -->
                </button>
            </div>
        </div>
    </div>

    <div class="container p-4 mt-4 container_global">
        <div class="row justify-content-center">
            <div class="col-auto">
                <video src="data_index/global_v4.mp4" class="video_global" controls loop>
                    <source type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
        </div>
    </div>
    <div class="container container-box-main border rounded p-4 mt-4 ">
        <div class="row justify-content-center">
            <div class="col-auto">
                <h3>Abstract</h3>
            </div>
        </div>
        <div class="row">
            <div class="col-auto">
                <p class="text-justify lead fs-6">We are aiming for Society 5.0 which emphasizes improving workplace quality of life through AI and robotics. However, current robots lack human-like situational understanding and often rely on pre-programmed tasks or supervised learning. Additionally, t here is a need for safety metrics that consider users' subjective safety perceptions. This thesis introduces GPTAlly, a system for safe human-robot collaboration using Large Language Models (LLMs) and Visual Language Models (VLMs). LLMs help infer users' subjective safety perceptions in collaborative tasks, influencing a Safety Index algorithm that adjusts safety evaluations. The system ensures robots stop to prevent harmful collisions and uses an LLM-based coding paradigm to determine subsequent actions, either autonomously or as per user preferences. The actions are implemented by an LLM, which shapes robotic arm trajectories by interpreting the natural language instructions of the user to suggest 3D poses. A user study compares safety perception scaling factors from GPT-4 with participants' estimates. The study also evaluates user satisfaction with the changes in robot behavior. The accuracy of the streamlined coding paradigm is evaluated through contextual experiments by varying the number of conditions processed by the LLM as well as paraphrasing the conditions. The satisfaction with the trajectories shaped from 3D poses is assessed through another user study. The study finds that LLMs effectively integrate human safety perceptions, with GPT-4's estimations closely matching user responses and participants expressing satisfaction with behavior changes. However, the coding paradigm's contextual accuracy can be below 50%. Finally, the robotic arm trajectories found that users preferred trajectories shaped by their natural language inputs over uninfluenced ones.</p>
            </div>
        </div>
    </div>


    <div class="container mt-2 mb-2 border rounded p-4 container-box-main">
        <div class="row justify-content-around">
            <div class="col text-center">
                <img id="exp_image" src="Figures/Main_fig.png" class="img-fluid" alt="Responsive image">
            </div>
        </div>
    </div>

    <div class="container container-box-main border rounded p-4 mt-4 ">
        <div class="row justify-content-center">
            <div class="col-auto">
                <h3>Safety Evaluation</h3>
            </div>
        </div>
        <div class="row justify-content-center mt-3">
            <div class="col-auto">
                <p class="text-justify lead fs-6">The first goal is to demonstrate the capacity of LLMs to estimate human safety perceptions through a scaling factor (GPT<f><sub>f</sub></f>). This is achieved by comparing the results from GPT-4 to the answers from a user study with the prompts listed <a href="safety_evaluation.html" class="text-warning">here.</a></p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col text-center">
                <img id="exp_image_GPT" src="Figures/Exp1_Results_1.png" alt="Responsive image">
            </div>
        </div>
        <div class="row mt-5 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead fs-6">Through the experiment detailed <a href="safety_evaluation.html" class="text-warning">here</a>, the next goal is to demonstrate how LLMs can integrate human perception of safety into safety assessments. This is achieved by analyzing the reaction and efficiency of the robot with different GPT<f><sub>f</sub></f> across a spectrum of scenarios, ranging from easy to challenging. The following results come from the easier experiment.</p>
            </div>
        </div>
        <div class="row justify-content-center align-items-center">
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img src="Figures/Exp1_1_1_with_images.png" class="img-fluid" alt="Responsive image">
            </div>
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img src="Figures/table.png" class="img-fluid" alt="Responsive image">
            </div>
        </div>
        <div class="row mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead fs-6">In the following qualitative assessment, the satisfaction of the user with the behavior of the robot with different input prompts is tested through a user study. They are
                    requested to assess their level of satisfaction on a scale of 1 to 5 (1 is very unsatisfied and 5 is very satisfied) regarding the new behavior of the robot in comparison to its neutral behavior (where GPT<f><sub>f</sub></f> = 1.0).</p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col text-center">
                <img id="exp_image_GPT" src="Figures/Exp1_Results_2.png" alt="Responsive image">
            </div>
        </div>
    </div>

    <div class="container container-box-main border rounded p-4 mt-4 ">
        <div class="row justify-content-center">
            <div class="col-auto">
                <h3>Decision Making</h3>
            </div>
        </div>
        <div class="row justify-content-center mt-3">
            <div class="col-auto">
                <p class="text-justify lead fs-6"> Three distinct tests are made to evaluate the viability of using LLMs as a streamlined coding paradigm. The first one consists of changing the natural language used to express the same condition on the input and thereby assessing the condition paraphrasing. Then, with language inputs that follow a similar structure, namely condition adjustment, different conditions are tested. The final step is to assess the scalability of the condition (condition scalability), gauging the system's capability to effectively manage increasingly intricate conditions. An extensive description of the experiment can be found <a href="decision_making.html" class="text-warning">here.</a></p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col text-center">
                <img id="exp_image_GPT" src="Figures/Exp3_Results.png" alt="Responsive image">
            </div>
        </div>
    </div>

    <div class="container container-box-main border rounded p-4 mt-4 ">
        <div class="row justify-content-center">
            <div class="col-auto">
                <h3>Robot Engine: withdrawal</h3>
            </div>
        </div>
        <div class="row justify-content-center mt-3">
            <div class="col-auto">
                <p class="text-justify lead fs-6"> The objective of this experiment is to evaluate the use of LLMs to shape trajectories. The methods' performance was evaluated in a user study, where a total of XX data points were collected from XX participants. Each participant was asked to rate the withdrawal methods on a 1-5 Likert scale, based on a given natural language input. The following figure summarizes the distribution of the answers for each method. Withdrawal 1 represents the withdrawal method proposed by <a href="decision_making.html" class="text-warning">Garcia et al.</a>, Withdrawal 2 is the same method with the parking position modified by GPT-4, and Withdrawal 3 is the method where GPT-4 suggests a new withdrawal position directly.</p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col text-center">
                <img id="exp_image_GPT" src="Figures/withdrawal.png" alt="Responsive image">
            </div>
        </div>
        <div class="row mt-5 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead fs-6">In the same user study, participants were asked to select their preferred withdrawal method if they were in the user's situation. On the left, the figure displays the average satisfaction level, ranging from 1 (very unsatisfied) to 5 (very satisfied). An analysis of variance (ANOVA) test and two t-tests were conducted to examine the differences between the means. In the same user study, participants were asked to select their preferred withdrawal method if they were in the user's situation as shown on the right. Unlike the previous question where they had to assess their satisfaction, they were asked to decide on only one favorite.</p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/withdrawal_r2.png" class="img-fluid" alt="Responsive image">
            </div>
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/withdrawal_choices.png" class="img-fluid" alt="Responsive image">
            </div>
        </div>

        <div class="row mt-5 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead fs-6">The users were also asked to assess the significance of each factor for their previous answers. The following figure displays the distribution of their responses. There were three factors they had to evaluate: safety, which refers to the robot moving to a safer location; compliance, which indicates the robot following the user's command; and human-like behavior, which involves the robot exhibiting behavior resembling that of a human. </p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col text-center">
                <img id="exp_image_GPT" src="Figures/withdrawal_factors.png" alt="Responsive image">
            </div>
        </div>
        <div class="row mt-5 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead fs-6">Finally, the distributions from the users were divided into three groups: students from UCLouvail who did not receive an engineering education (UCLouvain-Other), students from UCLouvain who did receive an engineering education (UCLouvain-Engineer), and students from Ritsumeikan who received engineering education (Ritsumeikan-Engineer). UCLouvain students study in Belgium, while Ritsumeikan students study in Japan</p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col text-center">
                <img id="exp_image_GPT" src="Figures/withdrawal_dataset_dist.png" alt="Responsive image">
            </div>
        </div>
    </div>

    <div class="container container-box-main border rounded p-4 mt-4 mb-4 ">
        <div class="row justify-content-center">
            <div class="col-auto">
                <h3>Robot Engine: Change Objective</h3>
            </div>
        </div>
        <div class="row justify-content-center mt-3">
            <div class="col-auto">
                <p class="text-justify lead fs-6"> With the same objective of evaluating the use of LLMs to shape trajectories, the change in objective is evaluated by comparing the responses provided by GPT-4 and users in a user study when given the same instructions, such as "Pick a block between me and the robot". The user, ("Me") is positioned in (0.7, 0.0) and the robot is positioned in (0.1,0.0). In red is the answer from GPT-4 and the frequency indicates the number of times a user chose this block relative to the maximum number of times any block was chosen. The experiment is explained more in detail <a href="functions.html" class="text-warning">here.</a></p>
            </div>
        </div>
        <div class="row mt-1 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead fs-6"> On the left side, the prompt provided to both the user and GPT-4 is “Take a block that is close to me”. On the right side, the prompt is “Take a block that is close to the robot”. </p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/CO_4.png" class="img-fluid" alt="Responsive image">
            </div>
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/CO_7.png" class="img-fluid" alt="Responsive image">
            </div>
        </div>
        <div class="row mt-5 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead fs-6">On the left side, the prompt is “Take a block on the left”, and on the right side, the prompt is “Take a cube on the right”.</p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/CO_1.png" class="img-fluid" alt="Responsive image">
            </div>
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/CO_2.png" class="img-fluid" alt="Responsive image">
            </div>
        </div>
        <div class="row mt-5 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead fs-6">On the left side, the prompt is “Take a block far from the robot”, and on the right side, the prompt is “Take one of the furthest block from the robot”. </p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/CO_8.png" class="img-fluid" alt="Responsive image">
            </div>
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/CO_9.png" class="img-fluid" alt="Responsive image">
            </div>
        </div>
    </div>
    
<!-- Bootstrap Bundle JS -->
<script
        src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>

<!-- Chart.js -->
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    

<!-- <script src="scripts/main.js"></script> -->


</body>
</html>
