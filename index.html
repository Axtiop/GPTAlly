<!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC"
          crossorigin="anonymous">
    
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet">
  

    <link rel="stylesheet" href="style/style.css">

    <title>Thesis Website</title>
</head>
<body>

    <nav class="navbar navbar-light bg-light-nav-custom">
        <div class="container-fluid">
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarText" aria-controls="navbarText" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarText">
            <ul class="navbar-nav me-auto mb-2 mb-lg-0">
            <li class="nav-item">
                <a class="nav-link active" href="index.html">Main</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="safety_evaluation.html">Safety</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="decision_making.html">Decision Making</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="functions.html">Functions</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="welcome_page.html">Survey</a>
            </li>
            </ul>
        </div>
        </div>
    </nav>
    
    <div class="container container-box-main border rounded p-4 mt-4 ">
        <div class="row justify-content-center">
            <div class="col-auto">
                <h1>GPTAlly</h1>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-auto">
                <h4> A Safety-Oriented System
                    for Human-Robot Collaboration
                    based on Foundation Models</h4>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-auto">
                <h5><span style="color: rgb(79, 79, 239);"> Brieuc Bastin<sup>1</sup>, Gustavo Alfonso Garcia Ricardez<sup>2</sup>, Lotfi El Hafi<sup>2</sup>, Renaud Ronsse<sup>1</sup>, Benoit Macq<sup>1</sup></span></h5>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-auto">
                <h6><sup>1</sup>UCLouvain, <sup>2</sup>Ritsumeikan</h6>
            </div>
        </div>
        <div class="row justify-content-center mt-3">
            <div class="col-auto">
                <a href="https://github.com/Axtiop/GPTAlly_codebase.git" class="btn btn-light btn-with-logo border">
                        <span>GitHub</span>
                        <i class="fab fa-github"></i> <!-- GitHub icon -->
                </a>
            </div>
            <div class="col-auto">
                <button class="btn btn-light btn-with-logo border">
                    <span>Paper</span>
                    <i class="fas fa-file-pdf"></i> <!-- GitHub icon -->
                </button>
            </div>
            <div class="col-auto">
                <button class="btn btn-light btn-with-logo border">
                    <span>Video</span>
                    <i class="fas fa-video"></i> <!-- GitHub icon -->
                </button>
            </div>
        </div>
    </div>

    <div class="container p-4 mt-4 container_global">
        <div class="row justify-content-center">
            <div class="col-auto">
                <video src="data_index/global_v3.mp4" class="video_global" controls loop>
                    <source type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
        </div>
    </div>
    <div class="container container-box-main border rounded p-4 mt-4 ">
        <div class="row justify-content-center">
            <div class="col-auto">
                <h3>Abstract</h3>
            </div>
        </div>
        <div class="row">
            <div class="col-auto">
                <p class="text-justify lead">Society 5.0 marks a transformative era where the focus shifts from mere productivity to prioritizing the quality of life in the workplace. For such a transformation to take place, robots still need great improvements in the domain of understanding human intentions and decision-making processes through natural language. However, trans- lating these instructions into robotic motion in the real world remains challenging. Traditionally, such translation is possible only for narrow tasks and can hardly be used for something it has not been trained for. Embracing the generalization of such collaboration, emerging models such as Large Language Models (LLMs) and Visual Language Models (VLMs) have shown interesting properties. In this context, the thesis introduces a zero-shot LLMs/VLMs-based architecture model for a human-robot collaborative task. The model inputs an overview of the board, an image of the user, and his oral commands to create a safe and adaptable system. This work investigates the incorporation of human perception of safety into safety evaluations, the viability of utilizing LLMs for decision-making in HRC by providing a streamlined coding paradigm, and the use of LLMs to shape trajectories by suggesting 3D poses based on natural language input. Instead of fine-tuning the model on known trajectories and Safety Index, as conventionally performed with Deep Learning (DL) models, we only use pre-trained models such as Detic, MediaPipe, or GPT-4.</p>
            </div>
        </div>
    </div>


    <div class="container mt-2 mb-2 border rounded p-4 container-box-main">
        <div class="row justify-content-around">
            <div class="col text-center">
                <img id="exp_image" src="Figures/Main_fig.png" class="img-fluid" alt="Responsive image">
            </div>
        </div>
    </div>

    <div class="container container-box-main border rounded p-4 mt-4 ">
        <div class="row justify-content-center">
            <div class="col-auto">
                <h3>Safety Evaluation</h3>
            </div>
        </div>
        <div class="row justify-content-center mt-3">
            <div class="col-auto">
                <p class="text-justify lead">The first goal is to demonstrate the capacity of LLMs to estimate human safety perceptions through a scaling factor (GPT<f><sub>f</sub></f>). This is achieved by comparing the results from GPT-4 to the answers from a user study with the prompts listed <a href="safety_evaluation.html" class="text-warning">here.</a></p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col text-center">
                <img id="exp_image_GPT" src="Figures/Exp1_Results_1.png" alt="Responsive image">
            </div>
        </div>
        <div class="row mt-5 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead">Through the experiment detailed <a href="safety_evaluation.html" class="text-warning">here</a>, the next goal is to demonstrate how LLMs can integrate human perception of safety into safety assessments. This is achieved by analyzing the reaction and efficiency of the robot with different GPT<f><sub>f</sub></f> across a spectrum of scenarios, ranging from easy to challenging. The following results come from the easier experiment.</p>
            </div>
        </div>
        <div class="row justify-content-center align-items-center">
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img src="Figures/Exp1_1_1_with_images.png" class="img-fluid" alt="Responsive image">
            </div>
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img src="Figures/table.png" class="img-fluid" alt="Responsive image">
            </div>
        </div>
        <div class="row mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead">In the following qualitative assessment, the satisfaction of the user with the behavior of the robot with different input prompts is tested through a user study. They are
                    requested to assess their level of satisfaction on a scale of 1 to 5 (1 is very unsatisfied and 5 is very satisfied) regarding the new behavior of the robot in comparison to its neutral behavior (where GPT<f><sub>f</sub></f> = 1.0).</p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col text-center">
                <img id="exp_image_GPT" src="Figures/Exp1_Results_2.png" alt="Responsive image">
            </div>
        </div>
    </div>

    <div class="container container-box-main border rounded p-4 mt-4 ">
        <div class="row justify-content-center">
            <div class="col-auto">
                <h3>Decision Making</h3>
            </div>
        </div>
        <div class="row justify-content-center mt-3">
            <div class="col-auto">
                <p class="text-justify lead"> Three distinct tests are made to evaluate the viability of using LLMs as a streamlined coding paradigm. The first one consists of changing the natural language used to express the same condition on the input and thereby assessing the condition paraphrasing. Then, with language inputs that follow a similar structure, namely condition adjustment, different conditions are tested. The final step is to assess the scalability of the condition (condition scalability), gauging the system's capability to effectively manage increasingly intricate conditions. An extensive description of the experiment can be found <a href="decision_making.html" class="text-warning">here.</a></p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col text-center">
                <img id="exp_image_GPT" src="Figures/Exp3_Results.png" alt="Responsive image">
            </div>
        </div>
    </div>

    <div class="container container-box-main border rounded p-4 mt-4 ">
        <div class="row justify-content-center">
            <div class="col-auto">
                <h3>Robot Engine: withdrawal</h3>
            </div>
        </div>
        <div class="row justify-content-center mt-3">
            <div class="col-auto">
                <p class="text-justify lead"> The objective of this experiment is to evaluate the use of LLMs to shape trajectories. The methods' performance was evaluated in a user study, where a total of XX data points were collected from XX participants. Each participant was asked to rate the withdrawal methods on a 1-5 Likert scale, based on a given natural language input. The following figure summarizes the distribution of the answers for each method. Withdrawal 1 represents the withdrawal method proposed by <a href="decision_making.html" class="text-warning">Garcia et al.</a>, Withdrawal 2 is the same method with the parking position modified by GPT-4, and Withdrawal 3 is the method where GPT-4 suggests a new withdrawal position directly.</p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col text-center">
                <img id="exp_image_GPT" src="Figures/withdrawal.png" alt="Responsive image">
            </div>
        </div>
        <div class="row mt-5 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead">In the same user study, participants were asked to select their preferred withdrawal method if they were in the user's situation. On the left, the figure displays the average satisfaction level, ranging from 1 (very unsatisfied) to 5 (very satisfied). An analysis of variance (ANOVA) test and two t-tests were conducted to examine the differences between the means. In the same user study, participants were asked to select their preferred withdrawal method if they were in the user's situation as shown on the right. Unlike the previous question where they had to assess their satisfaction, they were asked to decide on only one favorite.</p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/withdrawal_r2.png" class="img-fluid" alt="Responsive image">
            </div>
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/withdrawal_choices.png" class="img-fluid" alt="Responsive image">
            </div>
        </div>

        <div class="row mt-5 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead">The users were also asked to assess the significance of each factor for their previous answers. The following figure displays the distribution of their responses. There were three factors they had to evaluate: safety, which refers to the robot moving to a safer location; compliance, which indicates the robot following the user's command; and human-like behavior, which involves the robot exhibiting behavior resembling that of a human. </p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col text-center">
                <img id="exp_image_GPT" src="Figures/withdrawal_factors.png" alt="Responsive image">
            </div>
        </div>
        <div class="row mt-5 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead">Finally, the distributions from the users were divided into three groups: students from UCLouvail who did not receive an engineering education (UCLouvain-Other), students from UCLouvain who did receive an engineering education (UCLouvain-Engineer), and students from Ritsumeikan who received engineering education (Ritsumeikan-Engineer). UCLouvain students study in Belgium, while Ritsumeikan students study in Japan</p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col text-center">
                <img id="exp_image_GPT" src="Figures/withdrawal_dataset_dist.png" alt="Responsive image">
            </div>
        </div>
    </div>

    <div class="container container-box-main border rounded p-4 mt-4 mb-4 ">
        <div class="row justify-content-center">
            <div class="col-auto">
                <h3>Robot Engine: Change Objective</h3>
            </div>
        </div>
        <div class="row justify-content-center mt-3">
            <div class="col-auto">
                <p class="text-justify lead"> With the same objective of evaluating the use of LLMs to shape trajectories, the change in objective is evaluated by comparing the responses provided by GPT-4 and users in a user study when given the same instructions, such as "Pick a block between me and the robot". The user, ("Me") is positioned in (0.7, 0.0) and the robot is positioned in (0.1,0.0). In red is the answer from GPT-4 and the frequency indicates the number of times a user chose this block relative to the maximum number of times any block was chosen. The experiment is explained more in detail <a href="functions.html" class="text-warning">here.</a></p>
            </div>
        </div>
        <div class="row mt-1 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead"> On the left side, the prompt provided to both the user and GPT-4 is “Take a block that is close to me”. On the right side, the prompt is “Take a block that is close to the robot”. </p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/CO_4.png" class="img-fluid" alt="Responsive image">
            </div>
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/CO_7.png" class="img-fluid" alt="Responsive image">
            </div>
        </div>
        <div class="row mt-5 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead">On the left side, the prompt is “Take a block on the left”, and on the right side, the prompt is “Take a cube on the right”.</p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/CO_1.png" class="img-fluid" alt="Responsive image">
            </div>
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/CO_2.png" class="img-fluid" alt="Responsive image">
            </div>
        </div>
        <div class="row mt-5 mb-2 text-center">
            <div class="col text-center">
                <p class="text-justify lead">On the left side, the prompt is “Take a block far from the robot”, and on the right side, the prompt is “Take one of the furthest block from the robot”. </p>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/CO_8.png" class="img-fluid" alt="Responsive image">
            </div>
            <div class="col-md-6 col-sm-12 text-center mb-5">
                <img id="exp_image" src="Figures/CO_9.png" class="img-fluid" alt="Responsive image">
            </div>
        </div>
    </div>
    
<!-- Bootstrap Bundle JS -->
<script
        src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>

<!-- Chart.js -->
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<!-- <script src="scripts/main.js"></script> -->


</body>
</html>
